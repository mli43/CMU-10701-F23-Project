{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cf2774",
   "metadata": {},
   "source": [
    "# Reference: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954c4a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4591c9",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9def3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4302253723144531"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another easily recognizable form of ar technol...</td>\n",
       "      <td>une autre forme connue de ra est l ecran de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however since sao paulo is a big and scattered...</td>\n",
       "      <td>cependant comme sao paulo est une grande ville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this provision provides among other things tha...</td>\n",
       "      <td>cette provision prevoit notamment qu une deduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where would you expect to find a document that...</td>\n",
       "      <td>ou crois tu que tu trouveras un document qui t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at the request of agency staff additional comm...</td>\n",
       "      <td>a la demande du personnel de l office royal a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  another easily recognizable form of ar technol...   \n",
       "1  however since sao paulo is a big and scattered...   \n",
       "2  this provision provides among other things tha...   \n",
       "3  where would you expect to find a document that...   \n",
       "4  at the request of agency staff additional comm...   \n",
       "\n",
       "                                                  fr  \n",
       "0  une autre forme connue de ra est l ecran de vi...  \n",
       "1  cependant comme sao paulo est une grande ville...  \n",
       "2  cette provision prevoit notamment qu une deduc...  \n",
       "3  ou crois tu que tu trouveras un document qui t...  \n",
       "4  a la demande du personnel de l office royal a ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "if os.path.isfile('./data/valid_subset.csv'):\n",
    "    # cleaned, filtered by length, 10% dataset\n",
    "    df = pd.read_csv('./data/valid_subset.csv', index_col=False)\n",
    "    \n",
    "elif os.path.isfile('./data/valid_cleaned_data.csv'):\n",
    "    # cleaned, filtered by length dataset\n",
    "    df = pd.read_csv('./data/valid_cleaned_data.csv', index_col=False)\n",
    "    \n",
    "elif os.path.isfile('./data/cleaned_data.csv'):\n",
    "    # cleaned dataset\n",
    "    df = pd.read_csv('./data/cleaned_data.csv', index_col=False)\n",
    "else:\n",
    "\n",
    "    df = pd.read_csv('./data/en-fr.csv')\n",
    "\n",
    "end = time.time()\n",
    "display(end - start)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a32d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146993 entries, 0 to 146992\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   en      146993 non-null  object\n",
      " 1   fr      146993 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488d018",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5554f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67252ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02099299430847168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean data only if not available    \n",
    "\n",
    "start = time.time()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "if not os.path.isfile('./data/cleaned_data.csv'):\n",
    "    df['en'] = df['en'].apply(lambda x: normalizeString(str(x)))\n",
    "    df['fr'] = df['fr'].apply(lambda x: normalizeString(str(x)))\n",
    "    df.to_csv('./data/cleaned_data.csv', index=False)\n",
    "    \n",
    "end = time.time()\n",
    "display(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c2c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another easily recognizable form of ar technol...</td>\n",
       "      <td>une autre forme connue de ra est l ecran de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however since sao paulo is a big and scattered...</td>\n",
       "      <td>cependant comme sao paulo est une grande ville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this provision provides among other things tha...</td>\n",
       "      <td>cette provision prevoit notamment qu une deduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where would you expect to find a document that...</td>\n",
       "      <td>ou crois tu que tu trouveras un document qui t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at the request of agency staff additional comm...</td>\n",
       "      <td>a la demande du personnel de l office royal a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146988</th>\n",
       "      <td>it would be most beneficial and effective for ...</td>\n",
       "      <td>il serait tres avantageux et efficace pour le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146989</th>\n",
       "      <td>cost reductions particular importance shall be...</td>\n",
       "      <td>reductions des couts une importance particulie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146990</th>\n",
       "      <td>this inconsistent approach to marking had sign...</td>\n",
       "      <td>cette methode de cotation non uniforme a eu de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146991</th>\n",
       "      <td>testimony of lgen gervais transcripts vol</td>\n",
       "      <td>temoignage du lgne gervais transcriptions vol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146992</th>\n",
       "      <td>a reinsurance agreement between the fcic and t...</td>\n",
       "      <td>l accord de reassurance entre la fcic et les c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146993 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       en  \\\n",
       "0       another easily recognizable form of ar technol...   \n",
       "1       however since sao paulo is a big and scattered...   \n",
       "2       this provision provides among other things tha...   \n",
       "3       where would you expect to find a document that...   \n",
       "4       at the request of agency staff additional comm...   \n",
       "...                                                   ...   \n",
       "146988  it would be most beneficial and effective for ...   \n",
       "146989  cost reductions particular importance shall be...   \n",
       "146990  this inconsistent approach to marking had sign...   \n",
       "146991          testimony of lgen gervais transcripts vol   \n",
       "146992  a reinsurance agreement between the fcic and t...   \n",
       "\n",
       "                                                       fr  \n",
       "0       une autre forme connue de ra est l ecran de vi...  \n",
       "1       cependant comme sao paulo est une grande ville...  \n",
       "2       cette provision prevoit notamment qu une deduc...  \n",
       "3       ou crois tu que tu trouveras un document qui t...  \n",
       "4       a la demande du personnel de l office royal a ...  \n",
       "...                                                   ...  \n",
       "146988  il serait tres avantageux et efficace pour le ...  \n",
       "146989  reductions des couts une importance particulie...  \n",
       "146990  cette methode de cotation non uniforme a eu de...  \n",
       "146991      temoignage du lgne gervais transcriptions vol  \n",
       "146992  l accord de reassurance entre la fcic et les c...  \n",
       "\n",
       "[146993 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filter dataset by length\n",
    "MAX_LENGTH = 35\n",
    "\n",
    "if not os.path.isfile('./data/valid_cleaned_data.csv'):\n",
    "    df['en_len'] = df['en'].apply(lambda sent: len(sent.split(\" \")))\n",
    "    df['fr_len'] = df['fr'].apply(lambda sent: len(sent.split(\" \")))\n",
    "\n",
    "    df = df[df['en_len'] < MAX_LENGTH]\n",
    "    df = df[df['fr_len'] < MAX_LENGTH]\n",
    "    \n",
    "    df = df[['en', 'fr']]\n",
    "    \n",
    "    df.to_csv('./data/valid_cleaned_data.csv', index=False)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7757d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146993 entries, 0 to 146992\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   en      146993 non-null  object\n",
      " 1   fr      146993 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "### Prepare only subset of data\n",
    "frac = 0.1\n",
    "\n",
    "if not os.path.isfile('./data/valid_subset.csv'):\n",
    "    df_subset = df.sample(frac=frac)\n",
    "    df_subset.to_csv('./data/valid_subset.csv', index=False)\n",
    "else:\n",
    "    df_subset = df\n",
    "\n",
    "df_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08378580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [en, fr]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset[df_subset.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b045139",
   "metadata": {},
   "source": [
    "## Create helpers to construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc0fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e729fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146993"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b50e5",
   "metadata": {},
   "source": [
    "## Preparing Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995d6416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146993/146993 [00:05<00:00, 26945.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.003048419952393"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "#df_small = df_subset.sample(frac=0.1)\n",
    "\n",
    "\n",
    "def prepareData(df):\n",
    "    en_lang = Lang('en')\n",
    "    fr_lang = Lang('fr')\n",
    "    \n",
    "    en_vocab = []\n",
    "    fr_vocab = []\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], position=0, leave=True):\n",
    "        en_sent = row['en']\n",
    "        fr_sent = row['fr']\n",
    "        \n",
    "        en_vocab += en_sent.split(\" \")\n",
    "        fr_vocab += fr_sent.split(\" \")\n",
    "        \n",
    "    \n",
    "    # Construct word2index and index2word dicts for the two languages\n",
    "    en_vocab = set(en_vocab)\n",
    "    fr_vocab = set(fr_vocab)\n",
    "    \n",
    "    en_word2index = dict([(word, i+2) for i, word in enumerate(en_vocab)])\n",
    "    fr_word2index = dict([(word, i+2) for i, word in enumerate(fr_vocab)])\n",
    "    \n",
    "    en_index2word = {v: k for k, v in en_word2index.items()}\n",
    "    fr_index2word = {v: k for k, v in fr_word2index.items()}\n",
    "    \n",
    "    en_lang.word2index = en_word2index\n",
    "    fr_lang.word2index = fr_word2index\n",
    "    \n",
    "    en_lang.index2word.update(en_index2word)\n",
    "    fr_lang.index2word.update(fr_index2word)\n",
    "    \n",
    "    en_lang.n_words = len(en_lang.index2word.keys())\n",
    "    fr_lang.n_words = len(fr_lang.index2word.keys())\n",
    "    \n",
    "    return en_lang, fr_lang\n",
    "        \n",
    "\n",
    "start = time.time()\n",
    "en_lang, fr_lang = prepareData(df_subset)\n",
    "end = time.time() \n",
    "display(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8653b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attarder': 2,\n",
       " 'humble': 3,\n",
       " 'cpmite': 4,\n",
       " 'surevaluees': 5,\n",
       " 'prefere': 6,\n",
       " 'photochimie': 7,\n",
       " 'aylward': 8,\n",
       " 'parus': 9,\n",
       " 'ali': 10,\n",
       " 'netcom': 11,\n",
       " 'deniers': 12,\n",
       " 'souterrains': 13,\n",
       " 'sugars': 14,\n",
       " 'ecoulees': 15,\n",
       " 'padano': 16,\n",
       " 'strobiles': 17,\n",
       " 'strategijas': 18,\n",
       " 'paysagere': 19,\n",
       " 'valorisees': 20,\n",
       " 'prevedendo': 21,\n",
       " 'ipa': 22,\n",
       " 'cpa': 23,\n",
       " 'buy': 24,\n",
       " 'maugerville': 25,\n",
       " 'hematological': 26,\n",
       " 'preventable': 27,\n",
       " 'conjugales': 28,\n",
       " 'proud': 29,\n",
       " 'zielarskie': 30,\n",
       " 'biotechnologie': 31,\n",
       " 'larepe': 32,\n",
       " 'rhs': 33,\n",
       " 'verifiez': 34,\n",
       " 'delgado': 35,\n",
       " 'macronutriments': 36,\n",
       " 'carrol': 37,\n",
       " 'neoformees': 38,\n",
       " 'batteries': 39,\n",
       " 'lakefield': 40,\n",
       " 'emil': 41,\n",
       " 'costache': 42,\n",
       " 'eternelle': 43,\n",
       " 'aministration': 44,\n",
       " 'intramoleculaires': 45,\n",
       " 'tomodensitometrie': 46,\n",
       " 'deflates': 47,\n",
       " 'penh': 48,\n",
       " 'imprimeur': 49,\n",
       " 'kw': 50,\n",
       " 'lanieres': 51,\n",
       " 'dubia': 52,\n",
       " 'terribles': 53,\n",
       " 'incitait': 54,\n",
       " 'litterales': 55,\n",
       " 'nettoyeurs': 56,\n",
       " 'fournissaient': 57,\n",
       " 'pieges': 58,\n",
       " 'pathologiqe': 59,\n",
       " 'conditionnels': 60,\n",
       " 'harcelement': 61,\n",
       " 'selenocysteine': 62,\n",
       " 'bavarde': 63,\n",
       " 'rapid': 64,\n",
       " 'crite': 65,\n",
       " 'toles': 66,\n",
       " 'vident': 67,\n",
       " 'handicapant': 68,\n",
       " 'gosalvez': 69,\n",
       " 'longeant': 70,\n",
       " 'paix': 71,\n",
       " 'partager': 72,\n",
       " 'ranger': 73,\n",
       " 'eic': 74,\n",
       " 'nutritionnel': 75,\n",
       " 'positionnees': 76,\n",
       " 'phenazopyridine': 77,\n",
       " 'amendees': 78,\n",
       " 'mangeaient': 79,\n",
       " 'portees': 80,\n",
       " 'interconnecteur': 81,\n",
       " 'montrealaises': 82,\n",
       " 'valenzuela': 83,\n",
       " 'meritent': 84,\n",
       " 'collectees': 85,\n",
       " 'radiotelevision': 86,\n",
       " 'kazakhes': 87,\n",
       " 'forcees': 88,\n",
       " 'imprimable': 89,\n",
       " 'nicotianae': 90,\n",
       " 'deroulant': 91,\n",
       " 'amendemeats': 92,\n",
       " 'resserre': 93,\n",
       " 'devale': 94,\n",
       " 'rls': 95,\n",
       " 'subrogation': 96,\n",
       " 'raseux': 97,\n",
       " 'caa': 98,\n",
       " 'gommeux': 99,\n",
       " 'evader': 100,\n",
       " 'wing': 101,\n",
       " 'clsr': 102,\n",
       " 'cononus': 103,\n",
       " 'repandu': 104,\n",
       " 'pftscn': 105,\n",
       " 'crebb': 106,\n",
       " 'billettes': 107,\n",
       " 'assumera': 108,\n",
       " 'globulaire': 109,\n",
       " 'ambrosini': 110,\n",
       " 'hep': 111,\n",
       " 'edlat': 112,\n",
       " 'walking': 113,\n",
       " 'odlum': 114,\n",
       " 'mecanique': 115,\n",
       " 'denver': 116,\n",
       " 'verify': 117,\n",
       " 'bdc': 118,\n",
       " 'pedopornographie': 119,\n",
       " 'civieres': 120,\n",
       " 'ssiia': 121,\n",
       " 'inclueront': 122,\n",
       " 'corydalis': 123,\n",
       " 'culot': 124,\n",
       " 'cybernex': 125,\n",
       " 'mts': 126,\n",
       " 'ccpsc': 127,\n",
       " 'sac': 128,\n",
       " 'defensifs': 129,\n",
       " 'eff': 130,\n",
       " 'murets': 131,\n",
       " 'recombinante': 132,\n",
       " 'economie': 133,\n",
       " 'cihi': 134,\n",
       " 'lafortune': 135,\n",
       " 'middleton': 136,\n",
       " 'majorations': 137,\n",
       " 'oac': 138,\n",
       " 'jd': 139,\n",
       " 'rollie': 140,\n",
       " 'abstenue': 141,\n",
       " 'larochel': 142,\n",
       " 'oct': 143,\n",
       " 'bustard': 144,\n",
       " 'dissuader': 145,\n",
       " 'malade': 146,\n",
       " 'residence': 147,\n",
       " 'articlenews': 148,\n",
       " 'habituent': 149,\n",
       " 'delofp': 150,\n",
       " 'seme': 151,\n",
       " 'rowe': 152,\n",
       " 'asparagine': 153,\n",
       " 'envoyees': 154,\n",
       " 'eluder': 155,\n",
       " 'cardiopulmonaires': 156,\n",
       " 'kystes': 157,\n",
       " 'science': 158,\n",
       " 'dzb': 159,\n",
       " 'crystal': 160,\n",
       " 'axles': 161,\n",
       " 'denues': 162,\n",
       " 'zeneca': 163,\n",
       " 'solanum': 164,\n",
       " 'paea': 165,\n",
       " 'evie': 166,\n",
       " 'tinctorius': 167,\n",
       " 'pae': 168,\n",
       " 'aubepine': 169,\n",
       " 'tolson': 170,\n",
       " 'maya': 171,\n",
       " 'recenser': 172,\n",
       " 'incompatibilitks': 173,\n",
       " 'dillen': 174,\n",
       " 'than': 175,\n",
       " 'ortakov': 176,\n",
       " 'electromagnetique': 177,\n",
       " 'preaffranchie': 178,\n",
       " 'exploitait': 179,\n",
       " 'enjoins': 180,\n",
       " 'piquants': 181,\n",
       " 'plantus': 182,\n",
       " 'woodworking': 183,\n",
       " 'brinker': 184,\n",
       " 'tournesol': 185,\n",
       " 'cyclone': 186,\n",
       " 'habitez': 187,\n",
       " 'excl': 188,\n",
       " 'phenoloxydase': 189,\n",
       " 'simulans': 190,\n",
       " 'sharon': 191,\n",
       " 'albion': 192,\n",
       " 'lactating': 193,\n",
       " 'presumement': 194,\n",
       " 'vosters': 195,\n",
       " 'ror': 196,\n",
       " 'jovanovic': 197,\n",
       " 'polyalcools': 198,\n",
       " 'osg': 199,\n",
       " 'neutrophiles': 200,\n",
       " 'thermogenes': 201,\n",
       " 'attirant': 202,\n",
       " 'debrouillardise': 203,\n",
       " 'cukjati': 204,\n",
       " 'srie': 205,\n",
       " 'kalbach': 206,\n",
       " 'prawo': 207,\n",
       " 'theory': 208,\n",
       " 'hypertrophies': 209,\n",
       " 'myocardique': 210,\n",
       " 'doi': 211,\n",
       " 'pilotees': 212,\n",
       " 'surimposee': 213,\n",
       " 'inimaginable': 214,\n",
       " 'complainants': 215,\n",
       " 'kwawkwaw': 216,\n",
       " 'remarquer': 217,\n",
       " 'temperaments': 218,\n",
       " 'retard': 219,\n",
       " 'japon': 220,\n",
       " 'kate': 221,\n",
       " 'arcons': 222,\n",
       " 'filature': 223,\n",
       " 'scolarises': 224,\n",
       " 'regate': 225,\n",
       " 'appro': 226,\n",
       " 'mediterraneenne': 227,\n",
       " 'gaal': 228,\n",
       " 'groningen': 229,\n",
       " 'quietude': 230,\n",
       " 'esg': 231,\n",
       " 'resistant': 232,\n",
       " 'affutage': 233,\n",
       " 'enclume': 234,\n",
       " 'aejc': 235,\n",
       " 'ramifiee': 236,\n",
       " 'sols': 237,\n",
       " 'houses': 238,\n",
       " 'bouquet': 239,\n",
       " 'onyett': 240,\n",
       " 'hemson': 241,\n",
       " 'ougandais': 242,\n",
       " 'restrictifs': 243,\n",
       " 'eprouviez': 244,\n",
       " 'given': 245,\n",
       " 'prejudiciable': 246,\n",
       " 'malachie': 247,\n",
       " 'interventionnisme': 248,\n",
       " 'frison': 249,\n",
       " 'assassinats': 250,\n",
       " 'fecale': 251,\n",
       " 'eon': 252,\n",
       " 'indirecte': 253,\n",
       " 'climatologie': 254,\n",
       " 'rasles': 255,\n",
       " 'zarate': 256,\n",
       " 'rallonge': 257,\n",
       " 'fogasa': 258,\n",
       " 'minutieusement': 259,\n",
       " 'sentis': 260,\n",
       " 'chancroid': 261,\n",
       " 'aqueduc': 262,\n",
       " 'thylprop': 263,\n",
       " 'tetraedrique': 264,\n",
       " 'silice': 265,\n",
       " 'apcm': 266,\n",
       " 'ioana': 267,\n",
       " 'sousproduits': 268,\n",
       " 'tshishennuatsh': 269,\n",
       " 'brassard': 270,\n",
       " 'batr': 271,\n",
       " 'caso': 272,\n",
       " 'irriguee': 273,\n",
       " 'surestaries': 274,\n",
       " 'npf': 275,\n",
       " 'plein': 276,\n",
       " 'attendons': 277,\n",
       " 'jersey': 278,\n",
       " 'pottier': 279,\n",
       " 'goldstein': 280,\n",
       " 'attrayant': 281,\n",
       " 'ddr': 282,\n",
       " 'eleves': 283,\n",
       " 'coordinateurs': 284,\n",
       " 'perime': 285,\n",
       " 'upov': 286,\n",
       " 'tauranga': 287,\n",
       " 'kinedyne': 288,\n",
       " 'decompose': 289,\n",
       " 'conta': 290,\n",
       " 'complementary': 291,\n",
       " 'biooxydation': 292,\n",
       " 'gordon': 293,\n",
       " 'aiic': 294,\n",
       " '?ompi': 295,\n",
       " 'ccfp': 296,\n",
       " 'legislations': 297,\n",
       " 'cgm': 298,\n",
       " 'parsemes': 299,\n",
       " 'bino': 300,\n",
       " 'innocuite': 301,\n",
       " 'venitienne': 302,\n",
       " 'xxxiii': 303,\n",
       " 'ralit': 304,\n",
       " 'fisheries': 305,\n",
       " 'lesdites': 306,\n",
       " 'recommendes': 307,\n",
       " 'brian': 308,\n",
       " 'cotler': 309,\n",
       " 'moldaves': 310,\n",
       " 'm': 311,\n",
       " 'vaporisateur': 312,\n",
       " 'slemence': 313,\n",
       " 'admis': 314,\n",
       " 'pateux': 315,\n",
       " 'isss': 316,\n",
       " 'mikovic': 317,\n",
       " 'remplacera': 318,\n",
       " 'enume': 319,\n",
       " 'parrainer': 320,\n",
       " 'interstitielle': 321,\n",
       " 'successivement': 322,\n",
       " 'referes': 323,\n",
       " 'plebiscite': 324,\n",
       " 'hyperliens': 325,\n",
       " 'goutent': 326,\n",
       " 'regressent': 327,\n",
       " 'hebergees': 328,\n",
       " 'remettez': 329,\n",
       " 'refinancer': 330,\n",
       " 'mogess': 331,\n",
       " 'conviviaux': 332,\n",
       " 'malicieux': 333,\n",
       " 'kayakistes': 334,\n",
       " 'anormal': 335,\n",
       " 'nagoya': 336,\n",
       " 'gay': 337,\n",
       " 'negligeable': 338,\n",
       " 'permeables': 339,\n",
       " 'portant': 340,\n",
       " 'coproduits': 341,\n",
       " 'hexagonaux': 342,\n",
       " 'psycholangagier': 343,\n",
       " 'sprachunterrichts': 344,\n",
       " 'interpretation': 345,\n",
       " 'avertissaient': 346,\n",
       " 'takten': 347,\n",
       " 'comportementales': 348,\n",
       " 'decoupe': 349,\n",
       " 'ofsingle': 350,\n",
       " 'transmissions': 351,\n",
       " 'oami': 352,\n",
       " 'bonbons': 353,\n",
       " 'auriculaires': 354,\n",
       " 'logiciels': 355,\n",
       " 'retransfert': 356,\n",
       " 'prefectorales': 357,\n",
       " 'payes': 358,\n",
       " 'lamecow': 359,\n",
       " 'simhaz': 360,\n",
       " 'intersocietes': 361,\n",
       " 'tron': 362,\n",
       " 'attila': 363,\n",
       " 'contiendrait': 364,\n",
       " 'apparentent': 365,\n",
       " 'romanin': 366,\n",
       " 'reagir': 367,\n",
       " 'hierarchises': 368,\n",
       " 'monfils': 369,\n",
       " 'defaveur': 370,\n",
       " 'gallizioli': 371,\n",
       " 'bombarder': 372,\n",
       " 'sakala': 373,\n",
       " 'granules': 374,\n",
       " 'hayes': 375,\n",
       " 'radiodiffusees': 376,\n",
       " 'tigress': 377,\n",
       " 'voyageur': 378,\n",
       " 'programmation': 379,\n",
       " 'reloueraient': 380,\n",
       " 'stenographe': 381,\n",
       " 'enla': 382,\n",
       " 'decrivant': 383,\n",
       " 'coumarique': 384,\n",
       " 'arrets': 385,\n",
       " 'copablepharon': 386,\n",
       " 'dominique': 387,\n",
       " 'parkroyalhead': 388,\n",
       " 'pirater': 389,\n",
       " 'granulosa': 390,\n",
       " 'pretraitees': 391,\n",
       " 'multiethnique': 392,\n",
       " 'verificationfr': 393,\n",
       " 'bakan': 394,\n",
       " 'teen': 395,\n",
       " 'ejes': 396,\n",
       " 'abernathy': 397,\n",
       " 'catechol': 398,\n",
       " 'subdeleguer': 399,\n",
       " 'apprenez': 400,\n",
       " 'homosexualite': 401,\n",
       " 'hemphillia': 402,\n",
       " 'louisiane': 403,\n",
       " 'inhibitrice': 404,\n",
       " 'doucet': 405,\n",
       " 'bruant': 406,\n",
       " 'tuberculosis': 407,\n",
       " 'germinatifs': 408,\n",
       " 'oaag': 409,\n",
       " 'largage': 410,\n",
       " 'elstein': 411,\n",
       " 'renard': 412,\n",
       " 'indokenya': 413,\n",
       " 'having': 414,\n",
       " 'terrell': 415,\n",
       " 'wickstrom': 416,\n",
       " 'petrol': 417,\n",
       " 'postdoctorales': 418,\n",
       " 'taoiste': 419,\n",
       " 'paracapnia': 420,\n",
       " 'aki': 421,\n",
       " 'communiquerez': 422,\n",
       " 'hobbs': 423,\n",
       " 'porcin': 424,\n",
       " 'abeele': 425,\n",
       " 'attendance': 426,\n",
       " 'dumping': 427,\n",
       " 'vestes': 428,\n",
       " 'rkponse': 429,\n",
       " 'humique': 430,\n",
       " 'melissa': 431,\n",
       " 'localisent': 432,\n",
       " 'peacetime': 433,\n",
       " 'arcview': 434,\n",
       " 'nakaseke': 435,\n",
       " 'completion': 436,\n",
       " 'fest': 437,\n",
       " 'cardiopathies': 438,\n",
       " 'hydrotraite': 439,\n",
       " 'saccages': 440,\n",
       " 'ckik': 441,\n",
       " 'finlandefranceallemagnegrecehongrieislandeirlandeitalielettonieliechtensteinlituanieluxembourgmaltepays': 442,\n",
       " 'dysfonctionnel': 443,\n",
       " 'rao': 444,\n",
       " 'invagination': 445,\n",
       " 'gaies': 446,\n",
       " 'deere': 447,\n",
       " 'antologia': 448,\n",
       " 'aaaf': 449,\n",
       " 'talwin': 450,\n",
       " 'tebar': 451,\n",
       " 'chiffrera': 452,\n",
       " 'tccl': 453,\n",
       " 'manipulait': 454,\n",
       " 'gartska': 455,\n",
       " 'grayi': 456,\n",
       " 'mclaughlin': 457,\n",
       " 'rupestres': 458,\n",
       " 'iac': 459,\n",
       " 'sylviane': 460,\n",
       " 'conseilleres': 461,\n",
       " 'greer': 462,\n",
       " 'nptf': 463,\n",
       " 'alpines': 464,\n",
       " 'aeroglisseurs': 465,\n",
       " 'soulanges': 466,\n",
       " 'mercury': 467,\n",
       " 'explorera': 468,\n",
       " 'dechargeai': 469,\n",
       " 'supprimerait': 470,\n",
       " 'democratas': 471,\n",
       " 'haiti': 472,\n",
       " 'bratislava': 473,\n",
       " 'affectera': 474,\n",
       " 'imprevues': 475,\n",
       " 'millepertuis': 476,\n",
       " 'transversal': 477,\n",
       " 'globalive': 478,\n",
       " 'osoyoos': 479,\n",
       " 'segreteria': 480,\n",
       " 'emerson': 481,\n",
       " 'galeote': 482,\n",
       " 'prenegocies': 483,\n",
       " 'eidci': 484,\n",
       " 'traquet': 485,\n",
       " 'gypsy': 486,\n",
       " 'viaducs': 487,\n",
       " 'legation': 488,\n",
       " 'bustes': 489,\n",
       " 'jubile': 490,\n",
       " 'midwater': 491,\n",
       " '?': 492,\n",
       " 'uerre': 493,\n",
       " 'rzduit': 494,\n",
       " 'benzaldehyde': 495,\n",
       " 'woodland': 496,\n",
       " 'paradis': 497,\n",
       " 'interoperabilite': 498,\n",
       " 'lh': 499,\n",
       " 'morphine': 500,\n",
       " 'marquera': 501,\n",
       " 'radiographie': 502,\n",
       " 'envahissait': 503,\n",
       " 'gsrwsqqexmsr': 504,\n",
       " 'renouvelables': 505,\n",
       " 'suggerez': 506,\n",
       " 'visualiseur': 507,\n",
       " 'limitatives': 508,\n",
       " 'automatisee': 509,\n",
       " 'djc': 510,\n",
       " 'evase': 511,\n",
       " 'restaurant': 512,\n",
       " 'firi': 513,\n",
       " 'preferable': 514,\n",
       " 'mousse': 515,\n",
       " 'tardifs': 516,\n",
       " 'pousses': 517,\n",
       " 'concertes': 518,\n",
       " 'dgve': 519,\n",
       " 'unz': 520,\n",
       " 'pistilles': 521,\n",
       " 'enzyme': 522,\n",
       " 'ornl': 523,\n",
       " 'chamberlain': 524,\n",
       " 'darussalem': 525,\n",
       " 'encerclement': 526,\n",
       " 'decourageant': 527,\n",
       " 'diabetes': 528,\n",
       " 'bangladesh': 529,\n",
       " 'applicateur': 530,\n",
       " 'multivariee': 531,\n",
       " 'ironique': 532,\n",
       " 'auditorium': 533,\n",
       " 'bobines': 534,\n",
       " 'allegresse': 535,\n",
       " 'carria': 536,\n",
       " 'mimes': 537,\n",
       " 'nommant': 538,\n",
       " 'hydrazonyle': 539,\n",
       " 'differenciees': 540,\n",
       " 'contreplaque': 541,\n",
       " 'vln': 542,\n",
       " 'intergenerationnel': 543,\n",
       " 'lsn': 544,\n",
       " 'apartheid': 545,\n",
       " 'intronise': 546,\n",
       " 'conv': 547,\n",
       " 'agnes': 548,\n",
       " 'dioca': 549,\n",
       " 'etamines': 550,\n",
       " 'sterling': 551,\n",
       " 'mna': 552,\n",
       " 'regroupees': 553,\n",
       " 'pietons': 554,\n",
       " 'enroulees': 555,\n",
       " 'simplement': 556,\n",
       " 'bovis': 557,\n",
       " 'pyrazolone': 558,\n",
       " 'cahngement': 559,\n",
       " 'escc': 560,\n",
       " 'thf': 561,\n",
       " 'confinement': 562,\n",
       " 'crosshole': 563,\n",
       " 'organised': 564,\n",
       " 'nanakshahi': 565,\n",
       " 'deposait': 566,\n",
       " 'kap': 567,\n",
       " 'ecri': 568,\n",
       " 'gentilis': 569,\n",
       " 'compla': 570,\n",
       " 'inspectorat': 571,\n",
       " 'filed': 572,\n",
       " 'enjoy': 573,\n",
       " 'starphoenix': 574,\n",
       " 'initiales': 575,\n",
       " 'pere': 576,\n",
       " 'filetes': 577,\n",
       " 'litteralement': 578,\n",
       " 'kristian': 579,\n",
       " 'salubre': 580,\n",
       " 'uencer': 581,\n",
       " 'controle': 582,\n",
       " 'opposeront': 583,\n",
       " 'communaute': 584,\n",
       " 'empietent': 585,\n",
       " 'voyez': 586,\n",
       " 'ductions': 587,\n",
       " 'lavez': 588,\n",
       " 'frcpc': 589,\n",
       " 'interinstitutionnelles': 590,\n",
       " 'ternium': 591,\n",
       " 'echo': 592,\n",
       " 'akhmedovich': 593,\n",
       " 'iwaasa': 594,\n",
       " 'goin': 595,\n",
       " 'prelevements': 596,\n",
       " 'pouvaient': 597,\n",
       " 'rythme': 598,\n",
       " 'radiosensibles': 599,\n",
       " 'complaisance': 600,\n",
       " 'hulley': 601,\n",
       " 'icip': 602,\n",
       " 'braden': 603,\n",
       " 'polluted': 604,\n",
       " 'mcallister': 605,\n",
       " 'inacceptable': 606,\n",
       " 'ravagees': 607,\n",
       " 'suffira': 608,\n",
       " 'binh': 609,\n",
       " 'marmettes': 610,\n",
       " 'terrils': 611,\n",
       " 'ogilvie': 612,\n",
       " 'migrations': 613,\n",
       " 'mutilations': 614,\n",
       " 'invalidation': 615,\n",
       " 'proximal': 616,\n",
       " 'naegleria': 617,\n",
       " 'guevremont': 618,\n",
       " 'aneantir': 619,\n",
       " 'jubilee': 620,\n",
       " 'fleshless': 621,\n",
       " 'postharvest': 622,\n",
       " 'let': 623,\n",
       " 'epistasique': 624,\n",
       " 'nervation': 625,\n",
       " 'able': 626,\n",
       " 'essential': 627,\n",
       " 'batholite': 628,\n",
       " 'mould': 629,\n",
       " 'rehabilitation': 630,\n",
       " 'intermoleculaire': 631,\n",
       " 'reevaluee': 632,\n",
       " 'abrite': 633,\n",
       " 'guralnik': 634,\n",
       " 'electrochimique': 635,\n",
       " 'delivres': 636,\n",
       " 'irecteur': 637,\n",
       " 'certainement': 638,\n",
       " 'northgrave': 639,\n",
       " 'luo': 640,\n",
       " 'moravskoslezsko': 641,\n",
       " 'brigus': 642,\n",
       " 'ecca': 643,\n",
       " 'racial': 644,\n",
       " 'pbr': 645,\n",
       " 'frique': 646,\n",
       " 'vhe': 647,\n",
       " 'mienne': 648,\n",
       " 'laire': 649,\n",
       " 'renommes': 650,\n",
       " 'melville': 651,\n",
       " 'inventes': 652,\n",
       " 'adonne': 653,\n",
       " 'lagrangian': 654,\n",
       " 'ntre': 655,\n",
       " 'grievement': 656,\n",
       " 'dioxine': 657,\n",
       " 'installerent': 658,\n",
       " 'supraventriculaires': 659,\n",
       " 'landscapes': 660,\n",
       " 'wilmington': 661,\n",
       " 'moldus': 662,\n",
       " 'shelter': 663,\n",
       " 'tygar': 664,\n",
       " 'legge': 665,\n",
       " 'mytilus': 666,\n",
       " 'guidees': 667,\n",
       " 'gh': 668,\n",
       " 'dairy': 669,\n",
       " 'rappelees': 670,\n",
       " 'plasmalemme': 671,\n",
       " 'orgpct': 672,\n",
       " 'passionnant': 673,\n",
       " 'ivanov': 674,\n",
       " 'entrepositaires': 675,\n",
       " 'maintranches': 676,\n",
       " 'ribbe': 677,\n",
       " 'fantaisie': 678,\n",
       " 'articulaires': 679,\n",
       " 'fongique': 680,\n",
       " 'vicharbour': 681,\n",
       " 'suffisait': 682,\n",
       " 'ecmo': 683,\n",
       " 'bagambiire': 684,\n",
       " 'participa': 685,\n",
       " 'uradas': 686,\n",
       " 'balais': 687,\n",
       " 'neoperla': 688,\n",
       " 'embarcations': 689,\n",
       " 'cyclopropylbenzene': 690,\n",
       " 'serevent': 691,\n",
       " 'efmt': 692,\n",
       " 'abreviations': 693,\n",
       " 'comptaient': 694,\n",
       " 'feito': 695,\n",
       " 'collimateur': 696,\n",
       " 'verifiables': 697,\n",
       " 'epit': 698,\n",
       " 'uniforms': 699,\n",
       " 'nuisant': 700,\n",
       " 'lumsden': 701,\n",
       " 'adolescents': 702,\n",
       " 'fascicules': 703,\n",
       " 'charia': 704,\n",
       " 'drew': 705,\n",
       " 'declenchent': 706,\n",
       " 'oxalate': 707,\n",
       " 'crampes': 708,\n",
       " 'ith': 709,\n",
       " 'microbiologique': 710,\n",
       " 'adjacente': 711,\n",
       " 'eurosystem': 712,\n",
       " 'kantonalbank': 713,\n",
       " 'insulin': 714,\n",
       " 'imputes': 715,\n",
       " 'ptc': 716,\n",
       " 'verde': 717,\n",
       " 'dykes': 718,\n",
       " 'familiarisees': 719,\n",
       " 'rallument': 720,\n",
       " 'omc': 721,\n",
       " 'coiffes': 722,\n",
       " 'bcsi': 723,\n",
       " 'montcalm': 724,\n",
       " 'quete': 725,\n",
       " 'refused': 726,\n",
       " 'fernande': 727,\n",
       " 'dpe': 728,\n",
       " 'inaugral': 729,\n",
       " 'etty': 730,\n",
       " 'fitzjohn': 731,\n",
       " 'kyoto': 732,\n",
       " 'pompant': 733,\n",
       " 'description': 734,\n",
       " 'yougoslave': 735,\n",
       " 'tetracyclines': 736,\n",
       " 'lorentz': 737,\n",
       " 'ipamt': 738,\n",
       " 'campoli': 739,\n",
       " 'breaking': 740,\n",
       " 'perforation': 741,\n",
       " 'balle': 742,\n",
       " 'vido': 743,\n",
       " 'simplified': 744,\n",
       " 'osez': 745,\n",
       " 'outcome': 746,\n",
       " 'ah': 747,\n",
       " 'lazio': 748,\n",
       " 'encanteurs': 749,\n",
       " 'taxable': 750,\n",
       " 'red': 751,\n",
       " 'wilp': 752,\n",
       " 'sise': 753,\n",
       " 'gcac': 754,\n",
       " 'raymore': 755,\n",
       " 'sdi': 756,\n",
       " 'migre': 757,\n",
       " 'fies': 758,\n",
       " 'transactionnelle': 759,\n",
       " 'adresser': 760,\n",
       " 'demantelee': 761,\n",
       " 'colmatage': 762,\n",
       " 'loggerhead': 763,\n",
       " 'espace': 764,\n",
       " 'haultain': 765,\n",
       " 'occupation': 766,\n",
       " 'coquilliere': 767,\n",
       " 'dogme': 768,\n",
       " 'cordons': 769,\n",
       " 'autonoma': 770,\n",
       " 'monongahela': 771,\n",
       " 'sucre': 772,\n",
       " 'assembles': 773,\n",
       " 'paroxysmal': 774,\n",
       " 'asphyxie': 775,\n",
       " 'esoteriques': 776,\n",
       " 'sauraient': 777,\n",
       " 'balancer': 778,\n",
       " 'ramasses': 779,\n",
       " 'hongkong': 780,\n",
       " 'endurci': 781,\n",
       " 'fussent': 782,\n",
       " 'initialization': 783,\n",
       " 'megohm': 784,\n",
       " 'spfl': 785,\n",
       " 'maltese': 786,\n",
       " 'celibataire': 787,\n",
       " 'vendu': 788,\n",
       " 'homogeneise': 789,\n",
       " 'shapland': 790,\n",
       " 'concerto': 791,\n",
       " 'disease': 792,\n",
       " 'reproducteurs': 793,\n",
       " 'attirantes': 794,\n",
       " 'subissant': 795,\n",
       " 'cmdre': 796,\n",
       " 'rufiyaa': 797,\n",
       " 'morneault': 798,\n",
       " 'greves': 799,\n",
       " 'carl': 800,\n",
       " 'hollande': 801,\n",
       " 'msde': 802,\n",
       " 'indiscutable': 803,\n",
       " 'indissociables': 804,\n",
       " 'portes': 805,\n",
       " 'ailleu': 806,\n",
       " 'donneesbiographiques': 807,\n",
       " 'reniement': 808,\n",
       " 'india': 809,\n",
       " 'coentreprise': 810,\n",
       " 'mcardle': 811,\n",
       " 'eer': 812,\n",
       " 'layaute': 813,\n",
       " 'infligeaient': 814,\n",
       " 'lennax': 815,\n",
       " 'vegetaux': 816,\n",
       " 'choline': 817,\n",
       " 'numerisees': 818,\n",
       " 'variabilita': 819,\n",
       " 'lipsett': 820,\n",
       " 'europaischen': 821,\n",
       " 'solenoide': 822,\n",
       " 'ard': 823,\n",
       " 'pray': 824,\n",
       " 'prechauffeurs': 825,\n",
       " 'oxygenotherapie': 826,\n",
       " 'exportee': 827,\n",
       " 'commencaient': 828,\n",
       " 'categories': 829,\n",
       " 'itans': 830,\n",
       " 'kinnear': 831,\n",
       " 'niches': 832,\n",
       " 'parentale': 833,\n",
       " 'carcinomes': 834,\n",
       " 'indeniable': 835,\n",
       " 'rassasies': 836,\n",
       " 'prematurement': 837,\n",
       " 'progressive': 838,\n",
       " 'potable': 839,\n",
       " 'fcfa': 840,\n",
       " 'juga': 841,\n",
       " 'engaging': 842,\n",
       " 'holistique': 843,\n",
       " 'composeront': 844,\n",
       " 'cesarienne': 845,\n",
       " 'socioeconomic': 846,\n",
       " 'connaitra': 847,\n",
       " 'horizon': 848,\n",
       " 'tement': 849,\n",
       " 'concorde': 850,\n",
       " 'vernaculaires': 851,\n",
       " 'bdm': 852,\n",
       " 'pig': 853,\n",
       " 'jesuites': 854,\n",
       " 'rehrig': 855,\n",
       " 'carbocycle': 856,\n",
       " 'hyperprolactinemie': 857,\n",
       " 'marchetti': 858,\n",
       " 'excavateur': 859,\n",
       " 'fonorola': 860,\n",
       " 'taxer': 861,\n",
       " 'azerbaidjanaise': 862,\n",
       " 'vehiculer': 863,\n",
       " 'bridgewater': 864,\n",
       " 'timmermans': 865,\n",
       " 'incendiaires': 866,\n",
       " 'xviiieme': 867,\n",
       " 'kindermann': 868,\n",
       " 'nepriklausoma': 869,\n",
       " 'scotia': 870,\n",
       " 'glande': 871,\n",
       " 'quille': 872,\n",
       " 'ambae': 873,\n",
       " 'sergei': 874,\n",
       " 'felagslega': 875,\n",
       " 'cgb': 876,\n",
       " 'tonnage': 877,\n",
       " 'carat': 878,\n",
       " 'reconsidering': 879,\n",
       " 'interpolee': 880,\n",
       " 'multi': 881,\n",
       " 'faille': 882,\n",
       " 'mimaroglu': 883,\n",
       " 'productivas': 884,\n",
       " 'nasa': 885,\n",
       " 'ucrrp': 886,\n",
       " 'reevalue': 887,\n",
       " 'ouvrage': 888,\n",
       " 'lcsa': 889,\n",
       " 'injonction': 890,\n",
       " 'davidsonia': 891,\n",
       " 'autodefense': 892,\n",
       " 'incluaient': 893,\n",
       " 'epinephrine': 894,\n",
       " 'macrocarpa': 895,\n",
       " 'whan': 896,\n",
       " 'evf': 897,\n",
       " 'veillera': 898,\n",
       " 'legitimation': 899,\n",
       " 'convaincre': 900,\n",
       " 'indication': 901,\n",
       " 'wspa': 902,\n",
       " 'lampton': 903,\n",
       " 'lectriques': 904,\n",
       " 'word': 905,\n",
       " 'transmurale': 906,\n",
       " 'nucleoporine': 907,\n",
       " 'reflections': 908,\n",
       " 'carrefour': 909,\n",
       " 'passey': 910,\n",
       " 'coltes': 911,\n",
       " 'nourrissant': 912,\n",
       " 'bannir': 913,\n",
       " 'voudrions': 914,\n",
       " 'mycelienne': 915,\n",
       " 'microdonnees': 916,\n",
       " 'lotte': 917,\n",
       " 'alkylammoniums': 918,\n",
       " 'skip': 919,\n",
       " 'biotiques': 920,\n",
       " 'blais': 921,\n",
       " 'oleoduc': 922,\n",
       " 'ledit': 923,\n",
       " 'revenue': 924,\n",
       " 'demenageames': 925,\n",
       " 'telecoms': 926,\n",
       " 'sunshine': 927,\n",
       " 'ingenieuses': 928,\n",
       " 'lophopyrum': 929,\n",
       " 'norwegian': 930,\n",
       " 'broderles': 931,\n",
       " 'cupido': 932,\n",
       " 'priment': 933,\n",
       " 'cinematographique': 934,\n",
       " 'sinai': 935,\n",
       " 'carersmentalhealth': 936,\n",
       " 'infligees': 937,\n",
       " 'yards': 938,\n",
       " 'wish': 939,\n",
       " 'krahmer': 940,\n",
       " 'glissieres': 941,\n",
       " 'univalor': 942,\n",
       " 'sommet': 943,\n",
       " 'drdc': 944,\n",
       " 'transcanada': 945,\n",
       " 'securisees': 946,\n",
       " 'disenfranchised': 947,\n",
       " 'polledo': 948,\n",
       " 'provocateur': 949,\n",
       " 'inequienne': 950,\n",
       " 'affidavit': 951,\n",
       " 'clirninera': 952,\n",
       " 'coporal': 953,\n",
       " 'dpnce': 954,\n",
       " 'contexts': 955,\n",
       " 'validity': 956,\n",
       " 'lorzing': 957,\n",
       " 'savings': 958,\n",
       " 'contribuerait': 959,\n",
       " 'crisp': 960,\n",
       " 'determinants': 961,\n",
       " 'mitsuru': 962,\n",
       " 'antagoniste': 963,\n",
       " 'extends': 964,\n",
       " 'pccqe': 965,\n",
       " 'histamine': 966,\n",
       " 'accelerees': 967,\n",
       " 'travaillerent': 968,\n",
       " 'photocopieurs': 969,\n",
       " 'hauturiere': 970,\n",
       " 'storti': 971,\n",
       " 'porters': 972,\n",
       " 'habitions': 973,\n",
       " 'maintiennent': 974,\n",
       " 'reprise': 975,\n",
       " 'faq': 976,\n",
       " 'nageant': 977,\n",
       " 'tibbels': 978,\n",
       " 'oing': 979,\n",
       " 'detaillant': 980,\n",
       " 'reculera': 981,\n",
       " 'significantly': 982,\n",
       " 'courant': 983,\n",
       " 'spence': 984,\n",
       " 'provisionnel': 985,\n",
       " 'oserent': 986,\n",
       " 'covalents': 987,\n",
       " 'settling': 988,\n",
       " 'khrushcheva': 989,\n",
       " 'cervicaux': 990,\n",
       " 'locationacquisition': 991,\n",
       " 'distorsions': 992,\n",
       " 'controlera': 993,\n",
       " 'lindberg': 994,\n",
       " 'return': 995,\n",
       " 'secondaire': 996,\n",
       " 'denriere': 997,\n",
       " 'god': 998,\n",
       " 'light': 999,\n",
       " 'canadensis': 1000,\n",
       " 'smile': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd28a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>however since sao paulo is a big and scattered...</td>\n",
       "      <td>cependant comme sao paulo est une grande ville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at the request of agency staff additional comm...</td>\n",
       "      <td>a la demande du personnel de l office royal a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i firmly believe firefighters police paramedic...</td>\n",
       "      <td>je crois fermement que les pompiers policiers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a substantial amount of effort is devoted to m...</td>\n",
       "      <td>des efforts considerables sont consacres a l e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>link between apf and profitability strongly en...</td>\n",
       "      <td>l attention portee au csa pourrait se faire au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36708</th>\n",
       "      <td>date and place of next meeting monday th may f...</td>\n",
       "      <td>date et lieu de la prochaine reunion lundi mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36709</th>\n",
       "      <td>number of poor children on the rise</td>\n",
       "      <td>le nombre d enfants pauvres est a la hausse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36710</th>\n",
       "      <td>wildlife must move about to find food water an...</td>\n",
       "      <td>les especes sauvages doivent se deplacer a la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36711</th>\n",
       "      <td>he could not handle the pressure brought on by...</td>\n",
       "      <td>il n a pas resiste a la pression qu a apportee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36712</th>\n",
       "      <td>a reinsurance agreement between the fcic and t...</td>\n",
       "      <td>l accord de reassurance entre la fcic et les c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36713 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      however since sao paulo is a big and scattered...   \n",
       "1      at the request of agency staff additional comm...   \n",
       "2      i firmly believe firefighters police paramedic...   \n",
       "3      a substantial amount of effort is devoted to m...   \n",
       "4      link between apf and profitability strongly en...   \n",
       "...                                                  ...   \n",
       "36708  date and place of next meeting monday th may f...   \n",
       "36709                number of poor children on the rise   \n",
       "36710  wildlife must move about to find food water an...   \n",
       "36711  he could not handle the pressure brought on by...   \n",
       "36712  a reinsurance agreement between the fcic and t...   \n",
       "\n",
       "                                                      fr  \n",
       "0      cependant comme sao paulo est une grande ville...  \n",
       "1      a la demande du personnel de l office royal a ...  \n",
       "2      je crois fermement que les pompiers policiers ...  \n",
       "3      des efforts considerables sont consacres a l e...  \n",
       "4      l attention portee au csa pourrait se faire au...  \n",
       "...                                                  ...  \n",
       "36708  date et lieu de la prochaine reunion lundi mai...  \n",
       "36709        le nombre d enfants pauvres est a la hausse  \n",
       "36710  les especes sauvages doivent se deplacer a la ...  \n",
       "36711  il n a pas resiste a la pression qu a apportee...  \n",
       "36712  l accord de reassurance entre la fcic et les c...  \n",
       "\n",
       "[36713 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_subset.sample(frac=0.75)\n",
    "\n",
    "df_test = pd.concat([df_subset, df_train])\n",
    "df_test.drop_duplicates(keep=False, inplace=True)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4de2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def get_dataloader(batch_size, en_lang, fr_lang, df):\n",
    "    \n",
    "    n = len(df)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=n, position=0, leave=True):\n",
    "        en_sent = row[en_lang.name]\n",
    "        fr_sent = row[fr_lang.name]\n",
    "        \n",
    "        en_ids = indexesFromSentence(en_lang, en_sent)\n",
    "        fr_ids = indexesFromSentence(fr_lang, fr_sent)\n",
    "                \n",
    "        en_ids.append(EOS_token)\n",
    "        fr_ids.append(EOS_token)\n",
    "        \n",
    "        input_ids[idx, :len(en_ids)] = en_ids\n",
    "        target_ids[idx, :len(fr_ids)] = fr_ids\n",
    "        \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) #, num_workers=8)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0b5963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110245/110245 [00:05<00:00, 19092.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.945578336715698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[64008, 41462, 10746, 64008,   565, 35914, 45824, 35297,     1,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [45125, 43209, 45672, 27450, 30756, 44908, 10492, 65116, 44285, 23784,\n",
       "          30756, 38577, 49772, 30625, 29549, 45015, 39804,  4560,     1,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [62836, 53086, 22024, 26417, 10492, 10907, 35914,  2652, 65527, 37091,\n",
       "          43512, 49772, 24968,  9984, 53190, 23376, 19564, 22853, 19947, 26417,\n",
       "           6713, 50112, 49772, 22853, 19947, 26417, 61623, 50112,     1,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [41688, 37468, 12684, 32010, 11380, 43209, 55291, 23784, 30756, 12298,\n",
       "           3561, 51646, 27195, 66439, 54223, 35914, 38192, 35693, 61538, 61695,\n",
       "          35914, 62687,  7266, 26417, 25824,     1,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [67549, 30589,  5745,     1,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [55798, 66660, 29983, 22277, 66660, 24377, 44254,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [45125, 11556, 60437, 51699, 26417, 10492, 65137, 26417, 47966, 14435,\n",
       "          61538, 18667, 24378, 12357, 49772, 13310, 50726, 40235, 35778, 25199,\n",
       "           8950, 21660, 26417, 49772,     1,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [22553, 10492, 48067, 35914, 47612, 26417, 55499, 64008, 55798, 11001,\n",
       "          24175, 66660,  8947, 51519,   285,  1764, 64008, 10492,  9152, 24175,\n",
       "              1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]], device='cuda:0'),\n",
       " tensor([[56142, 68129,  6517, 48578, 68848, 37381, 41234, 72148, 53711, 49472,\n",
       "          27059, 12597, 19930, 13055, 39975, 19509, 65405, 33360, 70680, 10145,\n",
       "          72527, 66573,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [39950, 45730, 22676, 67726, 19930,  1003, 45555, 56749, 49472, 27059,\n",
       "          78992, 57019, 45555, 53454, 19509, 68962, 79900,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [21567, 19509, 39976, 39535, 33360, 11211, 19930, 32494, 54247, 24124,\n",
       "          19930,  6353, 71087, 62385, 45555, 55897, 19509,  8459, 41234, 19834,\n",
       "          24070, 79683, 37170, 24070, 19446, 21760,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [54247, 56142, 51129, 25374,  6339, 28588, 16051, 60916, 33864, 76150,\n",
       "          26035, 39950, 50891, 19509, 67586, 63175, 53594, 18228, 19930, 56142,\n",
       "          39103, 32385, 19930, 78534, 81321, 28462, 19509, 44929, 54247, 45555,\n",
       "           1496,  6795, 53594, 71163,     1],\n",
       "         [27390, 69236, 54247,  6339, 55842,     1,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [49472, 53711, 74167, 78534, 45555, 64278, 61683, 22938,     1,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [36916,  9972, 66416, 56142, 50688, 19930, 38053, 78534, 61997, 45555,\n",
       "          78415, 75848, 30606, 59657, 78534, 15636, 23751, 45555, 39254,     1,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [23082, 19509, 14046, 38158, 65166, 24070, 55019, 42465, 33864, 60616,\n",
       "          41234, 50528, 15096, 76337,  6339, 63894, 55019,     1,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]], device='cuda:0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_loader = get_dataloader(8, en_lang, fr_lang, df_train)\n",
    "end = time.time()\n",
    "\n",
    "display(end - start)\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c945e",
   "metadata": {},
   "source": [
    "## Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb4d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.LSTM(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c0e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.LSTM(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab26e7",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ced0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(train_loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd7b26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(dataloader, encoder, decoder, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Test')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_tensor, target_tensor = data\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(test_loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c079af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e05e9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, test_dataloader, encoder, decoder, n_epochs,\n",
    "          encoder_optimizer, decoder_optimizer, encoder_scheduler, decoder_scheduler,\n",
    "          criterion):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch} / {n_epochs}\")\n",
    "        \n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        test_loss = test_epoch(test_dataloader, encoder, decoder, criterion)\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"encoder lr = {encoder_scheduler.get_lr()}, decoder lr = {decoder_scheduler.get_lr()}\")\n",
    "        \n",
    "\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('%s (%d %d%%)' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100))\n",
    "\n",
    "    showPlot(train_losses)\n",
    "    showPlot(test_losses)\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90ed2a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac1a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110245/110245 [00:05<00:00, 19157.46it/s]\n",
      "100%|██████████| 36713/36713 [00:01<00:00, 19223.77it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = get_dataloader(batch_size, en_lang, fr_lang, df_train)\n",
    "test_loader = get_dataloader(batch_size, en_lang, fr_lang, df_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f079f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "encoder = EncoderRNN(en_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, fr_lang.n_words).to(device)\n",
    "\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, step_size=2, gamma=0.5)\n",
    "decoder_scheduler = optim.lr_scheduler.StepLR(decoder_optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40740989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [11:17<00:00,  5.09it/s, train_loss=3.4034]\n",
      "Test: 100%|██████████| 1148/1148 [01:06<00:00, 17.24it/s, test_loss=3.1413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [11:18<00:00,  5.08it/s, train_loss=2.9427]\n",
      "Test: 100%|██████████| 1148/1148 [01:06<00:00, 17.22it/s, test_loss=3.0235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [11:18<00:00,  5.08it/s, train_loss=2.6794]\n",
      "Test: 100%|██████████| 1148/1148 [01:06<00:00, 17.19it/s, test_loss=2.9686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [11:18<00:00,  5.08it/s, train_loss=2.5416]\n",
      "Test: 100%|██████████| 1148/1148 [01:06<00:00, 17.22it/s, test_loss=2.9654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [11:18<00:00,  5.08it/s, train_loss=2.3838]\n",
      "Test: 100%|██████████| 1148/1148 [01:06<00:00, 17.20it/s, test_loss=2.9590]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62m 4s (- 0m 0s) (5 100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "train_losses, test_losses = train(train_loader, test_loader, encoder, decoder, epochs,\n",
    "                                 encoder_optimizer, decoder_optimizer,\n",
    "                                 encoder_scheduler, decoder_scheduler,\n",
    "                                 criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5736f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = sum(test_losses) / len(test_losses)\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            'criterion': criterion\n",
    "            }, f'./checkpoints/checkpoint_testloss-{test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe73225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2023-11-15 12:10:21.009473\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "print(f\"Finished at {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2900e",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45386287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac53faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70017933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        print(f\"Testing {i+1} / {n}\")\n",
    "        pair = df_test.sample(1).values.tolist()[0]\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0], en_lang, fr_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceef831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aeb12580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 / 10\n",
      "> corporate income tax rate deductible percentage of existing resource allowance deductible percentage of royalties and mining taxes new tax credit for mineral exploration\n",
      "= taux d imposition des benefices des societes pourcentage deductible de la deduction relative a des ressources actuelle de pourcentage deductible des redevances et des impots miniers nouveau credit d impot pour exploration miniere\n",
      "< agence de sante canada et les services gouvernementaux canada et les premieres nations et les canadiennes en matiere de sante et de la sante <EOS>\n",
      "\n",
      "Testing 2 / 10\n",
      "> plants infected at the primary leaf stage display typical damping off disease symptoms\n",
      "= les pieds infectes au stade de la premiere feuille presentent habituellement les symptomes typiques de la fonte des semis\n",
      "< on a egalement fait l objet d une augmentation de la croissance des emissions de ges <EOS>\n",
      "\n",
      "Testing 3 / 10\n",
      "> this type of product is already emerging on the market\n",
      "= en fait ce genre de produit fait deja son entree sur le marche\n",
      "< les activites de recherche et de sauvetage <EOS>\n",
      "\n",
      "Testing 4 / 10\n",
      "> the government must ensure that medication prescribed is in fact provided and that a regular supply of appropriate medicines is guaranteed\n",
      "= le gouvernement doit prendre des dispositions pour que les traitements prescrits soient bien administres et pour que l approvisionnement des medicaments requis s effectue regulierement\n",
      "< la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode\n",
      "\n",
      "Testing 5 / 10\n",
      "> for example warmer soils appear to decrease concentrations of leachate and total inorganic nitrogen in soils although increased nitrogen uptake by vegetation masks these changes\n",
      "= par exemple des sols plus chauds semblent presenter des concentrations moindres d azote organique total et lixivie bien que l augmentation de l absorption d azote par la vegetation masque ces changements\n",
      "< plus grande combustion de la population canadienne de la population de l armee et de la population de l ile du prince edouard <EOS>\n",
      "\n",
      "Testing 6 / 10\n",
      "> the netherlands point b is replaced by the following\n",
      "= estonie le terme neant est remplace par les termes suivants\n",
      "< a la fin de l annee de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la periode de la\n",
      "\n",
      "Testing 7 / 10\n",
      "> partnerships for achieving independence\n",
      "= partnerships for achieving independence entraide feminine et partenariats pour l autonomie\n",
      "< a la conference annuelle de la conference annuelle de la conference annuelle de la recherche sur les jeunes a l intention de l industrie <EOS>\n",
      "\n",
      "Testing 8 / 10\n",
      "> the question of whether a positive list approach to scheduling commitments was inherently more transparent than a negative list approach was discussed\n",
      "= la question de savoir si une approche des listes positives pour l inscription d engagements dans des listes nationales etait fondamentalement plus transparente qu une approche des listes negatives a ete debattue\n",
      "< a la meme annee de la province de la province de la province de la region de la capitale nationale de la province de la province <EOS>\n",
      "\n",
      "Testing 9 / 10\n",
      "> authorities a new bill c received royal assent on thurdsday march\n",
      "= autorisations a nouveau le projet de loi c a recu la sanction royale le jeudi mars\n",
      "< la formation en matiere de sante et de la formation en matiere de sante et de la formation en matiere de sante et de la formation en anglais <EOS>\n",
      "\n",
      "Testing 10 / 10\n",
      "> wd brings a local perspective to local development issues\n",
      "= deo donne une perspective locale aux questions de developpement local\n",
      "< a un certain nombre de canadiens et de l industrie canadienne des premieres nations et des inuits <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6ebdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
