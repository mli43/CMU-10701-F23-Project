{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cf2774",
   "metadata": {},
   "source": [
    "# Reference: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954c4a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4591c9",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9def3088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.383059024810791"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another easily recognizable form of ar technol...</td>\n",
       "      <td>une autre forme connue de ra est l ecran de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however since sao paulo is a big and scattered...</td>\n",
       "      <td>cependant comme sao paulo est une grande ville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this provision provides among other things tha...</td>\n",
       "      <td>cette provision prevoit notamment qu une deduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where would you expect to find a document that...</td>\n",
       "      <td>ou crois tu que tu trouveras un document qui t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at the request of agency staff additional comm...</td>\n",
       "      <td>a la demande du personnel de l office royal a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  another easily recognizable form of ar technol...   \n",
       "1  however since sao paulo is a big and scattered...   \n",
       "2  this provision provides among other things tha...   \n",
       "3  where would you expect to find a document that...   \n",
       "4  at the request of agency staff additional comm...   \n",
       "\n",
       "                                                  fr  \n",
       "0  une autre forme connue de ra est l ecran de vi...  \n",
       "1  cependant comme sao paulo est une grande ville...  \n",
       "2  cette provision prevoit notamment qu une deduc...  \n",
       "3  ou crois tu que tu trouveras un document qui t...  \n",
       "4  a la demande du personnel de l office royal a ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "if os.path.isfile('./data/valid_subset.csv'):\n",
    "    # cleaned, filtered by length, 10% dataset\n",
    "    df = pd.read_csv('./data/valid_subset.csv', index_col=False)\n",
    "    \n",
    "elif os.path.isfile('./data/valid_cleaned_data.csv'):\n",
    "    # cleaned, filtered by length dataset\n",
    "    df = pd.read_csv('./data/valid_cleaned_data.csv', index_col=False)\n",
    "    \n",
    "elif os.path.isfile('./data/cleaned_data.csv'):\n",
    "    # cleaned dataset\n",
    "    df = pd.read_csv('./data/cleaned_data.csv', index_col=False)\n",
    "else:\n",
    "\n",
    "    df = pd.read_csv('./data/en-fr.csv')\n",
    "\n",
    "end = time.time()\n",
    "display(end - start)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a32d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146993 entries, 0 to 146992\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   en      146993 non-null  object\n",
      " 1   fr      146993 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488d018",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5554f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67252ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022000551223754883"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean data only if not available    \n",
    "\n",
    "start = time.time()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "if not os.path.isfile('./data/cleaned_data.csv'):\n",
    "    df['en'] = df['en'].apply(lambda x: normalizeString(str(x)))\n",
    "    df['fr'] = df['fr'].apply(lambda x: normalizeString(str(x)))\n",
    "    df.to_csv('./data/cleaned_data.csv', index=False)\n",
    "    \n",
    "end = time.time()\n",
    "display(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c2c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another easily recognizable form of ar technol...</td>\n",
       "      <td>une autre forme connue de ra est l ecran de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>however since sao paulo is a big and scattered...</td>\n",
       "      <td>cependant comme sao paulo est une grande ville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this provision provides among other things tha...</td>\n",
       "      <td>cette provision prevoit notamment qu une deduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where would you expect to find a document that...</td>\n",
       "      <td>ou crois tu que tu trouveras un document qui t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at the request of agency staff additional comm...</td>\n",
       "      <td>a la demande du personnel de l office royal a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146988</th>\n",
       "      <td>it would be most beneficial and effective for ...</td>\n",
       "      <td>il serait tres avantageux et efficace pour le ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146989</th>\n",
       "      <td>cost reductions particular importance shall be...</td>\n",
       "      <td>reductions des couts une importance particulie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146990</th>\n",
       "      <td>this inconsistent approach to marking had sign...</td>\n",
       "      <td>cette methode de cotation non uniforme a eu de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146991</th>\n",
       "      <td>testimony of lgen gervais transcripts vol</td>\n",
       "      <td>temoignage du lgne gervais transcriptions vol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146992</th>\n",
       "      <td>a reinsurance agreement between the fcic and t...</td>\n",
       "      <td>l accord de reassurance entre la fcic et les c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146993 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       en  \\\n",
       "0       another easily recognizable form of ar technol...   \n",
       "1       however since sao paulo is a big and scattered...   \n",
       "2       this provision provides among other things tha...   \n",
       "3       where would you expect to find a document that...   \n",
       "4       at the request of agency staff additional comm...   \n",
       "...                                                   ...   \n",
       "146988  it would be most beneficial and effective for ...   \n",
       "146989  cost reductions particular importance shall be...   \n",
       "146990  this inconsistent approach to marking had sign...   \n",
       "146991          testimony of lgen gervais transcripts vol   \n",
       "146992  a reinsurance agreement between the fcic and t...   \n",
       "\n",
       "                                                       fr  \n",
       "0       une autre forme connue de ra est l ecran de vi...  \n",
       "1       cependant comme sao paulo est une grande ville...  \n",
       "2       cette provision prevoit notamment qu une deduc...  \n",
       "3       ou crois tu que tu trouveras un document qui t...  \n",
       "4       a la demande du personnel de l office royal a ...  \n",
       "...                                                   ...  \n",
       "146988  il serait tres avantageux et efficace pour le ...  \n",
       "146989  reductions des couts une importance particulie...  \n",
       "146990  cette methode de cotation non uniforme a eu de...  \n",
       "146991      temoignage du lgne gervais transcriptions vol  \n",
       "146992  l accord de reassurance entre la fcic et les c...  \n",
       "\n",
       "[146993 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filter dataset by length\n",
    "MAX_LENGTH = 35\n",
    "\n",
    "if not os.path.isfile('./data/valid_cleaned_data.csv'):\n",
    "    df['en_len'] = df['en'].apply(lambda sent: len(sent.split(\" \")))\n",
    "    df['fr_len'] = df['fr'].apply(lambda sent: len(sent.split(\" \")))\n",
    "\n",
    "    df = df[df['en_len'] < MAX_LENGTH]\n",
    "    df = df[df['fr_len'] < MAX_LENGTH]\n",
    "    \n",
    "    df = df[['en', 'fr']]\n",
    "    \n",
    "    df.to_csv('./data/valid_cleaned_data.csv', index=False)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7757d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146993 entries, 0 to 146992\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   en      146993 non-null  object\n",
      " 1   fr      146993 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "### Prepare only subset of data\n",
    "frac = 0.1\n",
    "\n",
    "if not os.path.isfile('./data/valid_subset.csv'):\n",
    "    df_subset = df.sample(frac=frac)\n",
    "    df_subset.to_csv('./data/valid_subset.csv', index=False)\n",
    "else:\n",
    "    df_subset = df\n",
    "\n",
    "df_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08378580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [en, fr]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset[df_subset.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b045139",
   "metadata": {},
   "source": [
    "## Create helpers to construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc0fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e729fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146993"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b50e5",
   "metadata": {},
   "source": [
    "## Preparing Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995d6416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146993/146993 [00:05<00:00, 26688.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.06120753288269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "#df_small = df_subset.sample(frac=0.1)\n",
    "\n",
    "\n",
    "def prepareData(df):\n",
    "    en_lang = Lang('en')\n",
    "    fr_lang = Lang('fr')\n",
    "    \n",
    "    en_vocab = []\n",
    "    fr_vocab = []\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], position=0, leave=True):\n",
    "        en_sent = row['en']\n",
    "        fr_sent = row['fr']\n",
    "        \n",
    "        en_vocab += en_sent.split(\" \")\n",
    "        fr_vocab += fr_sent.split(\" \")\n",
    "        \n",
    "    \n",
    "    # Construct word2index and index2word dicts for the two languages\n",
    "    en_vocab = set(en_vocab)\n",
    "    fr_vocab = set(fr_vocab)\n",
    "    \n",
    "    en_word2index = dict([(word, i+2) for i, word in enumerate(en_vocab)])\n",
    "    fr_word2index = dict([(word, i+2) for i, word in enumerate(fr_vocab)])\n",
    "    \n",
    "    en_index2word = {v: k for k, v in en_word2index.items()}\n",
    "    fr_index2word = {v: k for k, v in fr_word2index.items()}\n",
    "    \n",
    "    en_lang.word2index = en_word2index\n",
    "    fr_lang.word2index = fr_word2index\n",
    "    \n",
    "    en_lang.index2word.update(en_index2word)\n",
    "    fr_lang.index2word.update(fr_index2word)\n",
    "    \n",
    "    en_lang.n_words = len(en_lang.index2word.keys())\n",
    "    fr_lang.n_words = len(fr_lang.index2word.keys())\n",
    "    \n",
    "    return en_lang, fr_lang\n",
    "        \n",
    "\n",
    "start = time.time()\n",
    "en_lang, fr_lang = prepareData(df_subset)\n",
    "end = time.time() \n",
    "display(end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd28a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>where would you expect to find a document that...</td>\n",
       "      <td>ou crois tu que tu trouveras un document qui t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i firmly believe firefighters police paramedic...</td>\n",
       "      <td>je crois fermement que les pompiers policiers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>impartial management and service to both priva...</td>\n",
       "      <td>la gestion impartiale et le service des intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at the end of the project these expenses have ...</td>\n",
       "      <td>al finalizar el proyecto hay que comprobar dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>once complete destroy the outdated certificate</td>\n",
       "      <td>delivrer les certificats de statut d indien en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36710</th>\n",
       "      <td>it can be varied in different ways</td>\n",
       "      <td>quel est le sentiment des perdants ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36711</th>\n",
       "      <td>in addition to federal income tax such individ...</td>\n",
       "      <td>en plus de l impot federal sur le revenu ces p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36712</th>\n",
       "      <td>bentonite perlite and zeolite as a soil amendm...</td>\n",
       "      <td>les cendres de bois doivent etre produites exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36713</th>\n",
       "      <td>testimony of lgen gervais transcripts vol</td>\n",
       "      <td>temoignage du lgne gervais transcriptions vol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36714</th>\n",
       "      <td>a reinsurance agreement between the fcic and t...</td>\n",
       "      <td>l accord de reassurance entre la fcic et les c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36715 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      where would you expect to find a document that...   \n",
       "1      i firmly believe firefighters police paramedic...   \n",
       "2      impartial management and service to both priva...   \n",
       "3      at the end of the project these expenses have ...   \n",
       "4         once complete destroy the outdated certificate   \n",
       "...                                                  ...   \n",
       "36710                 it can be varied in different ways   \n",
       "36711  in addition to federal income tax such individ...   \n",
       "36712  bentonite perlite and zeolite as a soil amendm...   \n",
       "36713          testimony of lgen gervais transcripts vol   \n",
       "36714  a reinsurance agreement between the fcic and t...   \n",
       "\n",
       "                                                      fr  \n",
       "0      ou crois tu que tu trouveras un document qui t...  \n",
       "1      je crois fermement que les pompiers policiers ...  \n",
       "2      la gestion impartiale et le service des intere...  \n",
       "3      al finalizar el proyecto hay que comprobar dic...  \n",
       "4      delivrer les certificats de statut d indien en...  \n",
       "...                                                  ...  \n",
       "36710               quel est le sentiment des perdants ?  \n",
       "36711  en plus de l impot federal sur le revenu ces p...  \n",
       "36712  les cendres de bois doivent etre produites exc...  \n",
       "36713      temoignage du lgne gervais transcriptions vol  \n",
       "36714  l accord de reassurance entre la fcic et les c...  \n",
       "\n",
       "[36715 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_subset.sample(frac=0.75)\n",
    "\n",
    "df_test = pd.concat([df_subset, df_train])\n",
    "df_test.drop_duplicates(keep=False, inplace=True)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4de2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def get_dataloader(batch_size, en_lang, fr_lang, df):\n",
    "    \n",
    "    n = len(df)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=n, position=0, leave=True):\n",
    "        en_sent = row[en_lang.name]\n",
    "        fr_sent = row[fr_lang.name]\n",
    "        \n",
    "        en_ids = indexesFromSentence(en_lang, en_sent)\n",
    "        fr_ids = indexesFromSentence(fr_lang, fr_sent)\n",
    "                \n",
    "        en_ids.append(EOS_token)\n",
    "        fr_ids.append(EOS_token)\n",
    "        \n",
    "        input_ids[idx, :len(en_ids)] = en_ids\n",
    "        target_ids[idx, :len(fr_ids)] = fr_ids\n",
    "        \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) #, num_workers=8)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c945e",
   "metadata": {},
   "source": [
    "## Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb4d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.LSTM(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        encoder_state = [torch.zeros(1, batch_size, self.hidden_size, device=device),\n",
    "                              torch.zeros(1, batch_size, self.hidden_size, device=device)]\n",
    "        \n",
    "        return encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c0e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.LSTM = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.LSTM(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab26e7",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ced0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        input_tensor, target_tensor = data\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(input_tensor.shape[0])\n",
    "        #print(encoder_hidden[0].shape, encoder_hidden[1].shape)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        #print(encoder_hidden[0].shape, encoder_hidden[1].shape)        \n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(train_loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd7b26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(dataloader, encoder, decoder, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Test')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_tensor, target_tensor = data\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(input_tensor.shape[0])\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(test_loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c079af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05e9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, test_dataloader, encoder, decoder, n_epochs,\n",
    "          encoder_optimizer, decoder_optimizer, encoder_scheduler, decoder_scheduler,\n",
    "          criterion):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch} / {n_epochs}\")\n",
    "        \n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        test_loss = test_epoch(test_dataloader, encoder, decoder, criterion)\n",
    "        \n",
    "        train_losses.append(loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"encoder lr = {encoder_scheduler.get_last_lr()}, decoder lr = {decoder_scheduler.get_last_lr()}\")\n",
    "        \n",
    "\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('%s (%d %d%%)' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100))\n",
    "\n",
    "    showPlot(train_losses)\n",
    "    showPlot(test_losses)\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ed2a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac1a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110245/110245 [00:05<00:00, 19110.37it/s]\n",
      "100%|██████████| 36715/36715 [00:01<00:00, 19184.64it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = get_dataloader(batch_size, en_lang, fr_lang, df_train)\n",
    "test_loader = get_dataloader(batch_size, en_lang, fr_lang, df_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f079f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "encoder = EncoderRNN(en_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, fr_lang.n_words).to(device)\n",
    "\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, step_size=1, gamma=0.95)\n",
    "decoder_scheduler = optim.lr_scheduler.StepLR(decoder_optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40740989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [09:39<00:00,  5.95it/s, train_loss=3.4657]\n",
      "Test: 100%|██████████| 1148/1148 [00:50<00:00, 22.90it/s, test_loss=3.2169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder lr = [0.01], decoder lr = [0.01]\n",
      "Epoch 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [09:39<00:00,  5.95it/s, train_loss=3.0276]\n",
      "Test: 100%|██████████| 1148/1148 [00:58<00:00, 19.48it/s, test_loss=3.0996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder lr = [0.0095], decoder lr = [0.0095]\n",
      "Epoch 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [10:47<00:00,  5.33it/s, train_loss=2.8329]\n",
      "Test: 100%|██████████| 1148/1148 [00:59<00:00, 19.26it/s, test_loss=3.0512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder lr = [0.009025], decoder lr = [0.009025]\n",
      "Epoch 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [10:44<00:00,  5.34it/s, train_loss=2.6921]\n",
      "Test: 100%|██████████| 1148/1148 [00:59<00:00, 19.27it/s, test_loss=3.0297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder lr = [0.00857375], decoder lr = [0.00857375]\n",
      "Epoch 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3446/3446 [10:44<00:00,  5.35it/s, train_loss=2.5787]\n",
      "Test: 100%|██████████| 1148/1148 [00:59<00:00, 19.27it/s, test_loss=3.0162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder lr = [0.0081450625], decoder lr = [0.0081450625]\n",
      "56m 22s (- 0m 0s) (5 100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "train_losses, test_losses = train(train_loader, test_loader, encoder, decoder, epochs,\n",
    "                                 encoder_optimizer, decoder_optimizer,\n",
    "                                 encoder_scheduler, decoder_scheduler,\n",
    "                                 criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ecada55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = sum(test_losses) / len(test_losses)\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            'criterion': criterion\n",
    "            }, f'./checkpoints/checkpoint_epoch{epochs}_testloss{test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfe73225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2023-11-16 21:32:22.642647\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "print(f\"Finished at {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2900e",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45386287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac53faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(input_tensor.shape[0])\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70017933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        print(f\"Testing {i+1} / {n}\")\n",
    "        pair = df_test.sample(1).values.tolist()[0]\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0], en_lang, fr_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aeb12580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 / 10\n",
      "> franchise holders\n",
      "= organismes detenteurs de droits de jeux\n",
      "< mycobacteries de la population canadienne de la colombie britannique manitoba ontario <EOS>\n",
      "\n",
      "Testing 2 / 10\n",
      "> most women in kakamega and makueni are engaged in unpaid family labour mainly farming which is unreliable because it depends on weather conditions\n",
      "= la plupart des femmes des deux districts travaillent au sein de la famille sans etre remunerees principalement dans la production agricole activite aleatoire car elle est tributaire des conditions meteorologiques\n",
      "< le comite de gestion des ressources humaines est le plus important de la population canadienne et de la gestion des ressources humaines <EOS>\n",
      "\n",
      "Testing 3 / 10\n",
      "> the rural to urban shift has had particularly significant impact on the province and there is still more work that needed to be done on the issue\n",
      "= le passage du rural a l urbain a eu des incidences particulierement importantes sur la province et il reste encore beaucoup de travail a faire sur la question\n",
      "< le programme de gestion des ressources humaines est le plus important de la population et la gestion des ressources humaines <EOS>\n",
      "\n",
      "Testing 4 / 10\n",
      "> the applicant submitted that the same apportioning concept should apply even in the absence of federal funding\n",
      "= la demanderesse soutenait qu il fallait appliquer ce pourcentage meme s il n y avait pas de subvention federale\n",
      "< le programme de prevention des ressources humaines a l intention des gouvernements provinciaux et territoriaux et provinciaux <EOS>\n",
      "\n",
      "Testing 5 / 10\n",
      "> many of these materials are also available in major research libraries in ontario and canada\n",
      "= beaucoup d entre eux sont egalement disponibles dans les principales bibliotheques de recherche de l ontario et du canada\n",
      "< le programme de gestion des ressources humaines et des programmes de gestion des ressources humaines et des ressources humaines <EOS>\n",
      "\n",
      "Testing 6 / 10\n",
      "> cette publication est disponible en francais sous le titre de l inventaire national des rejets de polluants devez vous produire une declaration ?\n",
      "= this publication is available in english under the title national pollutant release inventory are you required to report ?\n",
      "< le canada a fait l objet d un an pour le canada et le canada a l etranger <EOS>\n",
      "\n",
      "Testing 7 / 10\n",
      "> title information society chapter administrative expenditure of policy area information society the detail of articles and can be found in chapter xx title chapter article item\n",
      "= titre societe de l information chapitre depenses administratives du domaine politique societe de l information le detail des articles et se trouve dans le chapitre xx titre chapitre article poste\n",
      "< la gestion des ressources humaines et des ressources humaines a l intention des gouvernements provinciaux et territoriaux et provinciaux et territoriaux et provinciaux et territoriaux <EOS>\n",
      "\n",
      "Testing 8 / 10\n",
      "> decisions are published within a short period of time following the public hearing\n",
      "= les decisions sont publiees rapidement suivant l audience publique\n",
      "< sante canada maladies explorateur <EOS>\n",
      "\n",
      "Testing 9 / 10\n",
      "> the version of eurocode endorsed the use of the version of the comite euro international du beton ceb model code mc shrinkage and creep equations\n",
      "= la version du eurocode adopte l utilisation des equations du retrait et du fluage contenues dans la version du ceb model code de mc\n",
      "< la gestion des ressources humaines et des services de sante publique et de la gestion des ressources humaines <EOS>\n",
      "\n",
      "Testing 10 / 10\n",
      "> to avoid these risks extensive dialogue is planned on the programming and implementation of the sectoral policy in particular in the field of control and monitoring\n",
      "= afin d eviter ces risques il est prevu un dialogue soutenu sur la programmation et la mise en uvre de la politique sectorielle notamment dans le domaine du controle et de la surveillance\n",
      "< le programme de formation et de la sante publique est un programme de formation et de la gestion des ressources humaines <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31f6ebdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80580"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_lang.word2index['viole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87358e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
